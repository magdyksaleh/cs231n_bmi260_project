{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import skorch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 40000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=40000, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)\n",
    "\n",
    "for t, (x, y) in enumerate(loader_train):\n",
    "            x = x.to(device=device, dtype=dtype)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            \n",
    "for t, (x, y) in enumerate(loader_test):\n",
    "            x_Test = x.to(device=device, dtype=dtype)\n",
    "            y_Test = y.to(device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "x = np.load(\"X.npy\")\n",
    "y = np.load(\"y.npy\")\n",
    "\n",
    "x_full = torch.from_numpy(x)\n",
    "y_full = torch.from_numpy(y)\n",
    "x_full = x_full.to(device=device, dtype=dtype)\n",
    "y_full = y_full.to(device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "x = x_full[:,:,:,30:]\n",
    "y = y_full[30:]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "x_Test = x_full[:,:,:,0:29]\n",
    "y_Test = y_full[0:29]\n",
    "\n",
    "print(x_Test.shape)\n",
    "print(y_Test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_1 = 32\n",
    "channel_2 = 64\n",
    "channel_3 = 32\n",
    "\n",
    "\n",
    "learning_rate = 2.5e-3\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "in_channel = 3\n",
    "num_classes = 10\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channel,channel_1, kernel_size=5, padding=2, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(num_features= channel_1),\n",
    "    nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(num_features= channel_2),\n",
    "    nn.MaxPool2d(kernel_size=1,stride=1),\n",
    "    nn.Conv2d(channel_2, channel_3, kernel_size=3, padding=1, stride=1),\n",
    "    nn.ReLU(),\n",
    "    Flatten(),\n",
    "    nn.Linear(channel_3 *32*32, num_classes)\n",
    ")\n",
    "model = model.to(device=device)\n",
    "\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    module=model,\n",
    "    criterion = nn.CrossEntropyLoss,\n",
    "    optimizer=optim.Adam,\n",
    "#     optimizer_momentum = 0.9,\n",
    "    train_split=None,\n",
    "    max_epochs=5,\n",
    "    lr= learning_rate,\n",
    "    warm_start = True,\n",
    "    device = device\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "# for epoch in range(5):\n",
    "kf = KFold(n_splits=3, shuffle = True)\n",
    "accuracies=[]\n",
    "for train_index, test_index in kf.split(x):\n",
    "    accuracies=[]\n",
    "    print(train_index)\n",
    "    print(test_index)\n",
    "    xk_train, xk_test = x[train_index], x[test_index]\n",
    "    yk_train, yk_test = y[train_index], y[test_index]\n",
    "    net.fit(xk_train,yk_train)\n",
    "    y_pred = net.predict(xk_test)\n",
    "    acc = metrics.accuracy_score(yk_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    print('FinalAccuracy %.4f' % (np.mean(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = net.predict(x_Test)\n",
    "acc = metrics.accuracy_score(y_Test, y_pred_test)\n",
    "print('TestAccuracy %.4f' % (acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
